{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e6533b-06cb-424e-8b11-19ed72d358bc",
   "metadata": {},
   "source": [
    "# Classification (step through)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708f50d-a6ff-4f5f-9c80-97e05fa392f5",
   "metadata": {},
   "source": [
    "# Fellow the Megadector Classification repo first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752221cb-b80f-4a94-b3c7-baedaa6f9fdd",
   "metadata": {},
   "source": [
    "# Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a888d2cd-aacc-42c8-ac04-05b65ad7b428",
   "metadata": {},
   "source": [
    "This README describes how to train and run an animal \"species\" classifier. \"Species\" is in quotes, because the classifier can be trained to identify animals at arbitrary levels within the biological taxonomy of animals.\n",
    "\n",
    "This guide is written for internal use at Microsoft AI for Earth. Certain services, such as MegaDB and various private repos are only accessible interally within Microsoft. However, this guide may still be of interest to more technical users of the AI for Earth Camera Trap services.\n",
    "\n",
    "The classifiers trained with this pipeline are intended to be used in conjunction with MegaDetector, i.e., we use MegaDetector to find animals and crop them out, and we train/run our classifiers on those crops.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c433f-9a29-4938-9937-9d2dbc72d964",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876f29f7-9a96-43d7-bc07-32b304cf08ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Install Environment through yml file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5f23d5-4fa7-4377-b996-b77ec14a47e2",
   "metadata": {},
   "source": [
    "Install Anaconda or miniconda3. Then create the conda environment using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59be299f-9e3f-46c6-8384-17f287b09118",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -n cameratraps-classifier\n",
    "!conda activate cameratraps-classifier\n",
    "!cd git/cameratraps/\n",
    "!conda env update -f environment-classifier.yml --prune\n",
    "#could be for a while - about 30 mins "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e72762-aad7-4b6f-8fdc-fa9be2200ea2",
   "metadata": {},
   "source": [
    "### 2.Verifying that CUDA is available (and dealing with the case where it isn't)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b6a6d5-9523-482b-a616-e2170a0c4834",
   "metadata": {},
   "source": [
    "Verify that CUDA is available (assumes that the current working directory is the CameraTraps repo root):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1204f-f425-4080-9326-ae3686523353",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python sandbox/torch_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c331f65-6604-4a0d-b427-d3f2bbc8cbde",
   "metadata": {},
   "source": [
    "If CUDA isn't available but should be (i.e., you have an NVIDIA GPU and recent drivers)...\n",
    "it would return : `CUDA available: False`\n",
    "\n",
    "YMMV, but in at least one Linux environment, the following fixed this issue:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a3fcc8-fea8-4559-9a69-690396729d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch torchvision\n",
    "!conda install pytorch=1.10.1 torchvision=0.11.2 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a0abb9-1bcf-485d-8f50-019f4b8d66e3",
   "metadata": {},
   "source": [
    "### 3. Optional steps to make classification faster in Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b688b42-32d7-4ded-bbd4-d4149b675a70",
   "metadata": {},
   "source": [
    "If you are on Linux, you may also get some speedup by installing the accimage package for acclerated image loading. Because this is Linux-only and optional, we have commented it out of the environment file, but you can install it with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ec4a3-f619-4573-aa61-025ddd6510a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge accimage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41782589-a6d9-4c47-a69f-43203b0ef19a",
   "metadata": {},
   "source": [
    "Similarly, on Linux, you may get some speedup by installing Pillow-SIMD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5711923-8dcd-43da-8d33-b631c8bdfd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y pillow\n",
    "!pip install pillow-simd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a369adc-71de-40cd-b7f4-22bb2df148d8",
   "metadata": {},
   "source": [
    "### 4. Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f292c-45b2-4cc1-aec7-c352dff03eb1",
   "metadata": {},
   "source": [
    "The following environment variables are useful to have in `.bashrc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea5602eb-88e9-4e4a-98a6-2dc9d3896e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python development\n",
    "export PYTHONPATH=\"/path/to/repos/CameraTraps:/path/to/repos/ai4eutils\"\n",
    "export MYPYPATH=$PYTHONPATH\n",
    "\n",
    "# accessing MegaDB\n",
    "export COSMOS_ENDPOINT=\"[INTERNAL_USE]\"\n",
    "export COSMOS_KEY=\"[INTERNAL_USE]\"\n",
    "\n",
    "# running Batch API\n",
    "export BATCH_DETECTION_API_URL=\"http://[INTERNAL_USE]/v3/camera-trap/detection-batch\"\n",
    "export CLASSIFICATION_BLOB_STORAGE_ACCOUNT=\"[INTERNAL_USE]\"\n",
    "export CLASSIFICATION_BLOB_CONTAINER=\"classifier-training\"\n",
    "export CLASSIFICATION_BLOB_CONTAINER_WRITE_SAS=\"[INTERNAL_USE]\"\n",
    "export DETECTION_API_CALLER=\"[INTERNAL_USE]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf52319-1c10-4cea-bfe4-264f1b305c19",
   "metadata": {},
   "source": [
    "# AniMl repo Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81c244b-e136-4a2b-a463-5d36aad05923",
   "metadata": {},
   "source": [
    "\n",
    "#### Clone relevant repos and build Conda environments\n",
    "\n",
    "To clone the necessary repos, navigate to the `studio-lab-user`'s `home` directory, and from the sidebar menu select the Git icon > \"Clone A Repository\". Enter the Git repo URL for the [microsoft/CameraTraps](https://github.com/microsoft/CameraTraps) repo (https://github.com/microsoft/CameraTraps.git), and click \"Clone\". If you had the \"Search for environment.yml and build Conda environment.\" Box checked, it should automatically build the `cameratraps` Conda environment.\n",
    "\n",
    "Follow the same steps to clone the\n",
    "- [microsoft/ai4eutils](https://github.com/microsoft/ai4eutils) repo\n",
    "- [animl-analytics](https://github.com/tnc-ca-geo/animl-analytics) repo\n",
    "- and this ([animl-ml](https://github.com/tnc-ca-geo/animl-ml)) repo\n",
    "\n",
    "Next, navigate to `~/Cameratraps/` project root directory and run `conda env update -f environment-classifier.yml --prune` to build the `cameratraps-classifier` Conda environment, which is the primary one we'll be using.\n",
    "\n",
    "Finally, activate the `cameratraps-classifier` env and install `azure-cosmos` dependency (it's required but seemed to be missing from the env):\n",
    "\n",
    "```\n",
    "conda activate cameratraps-classifier\n",
    "conda install -n cameratraps-classifier -c conda-forge azure-cosmos\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0417d7-ec76-4662-b31b-ff406391b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ~ \n",
    "!git clone https://github.com/microsoft/ai4eutils ai4eutils\n",
    "!git clone https://github.com/tnc-ca-geo/animl-analytics animl-analytics\n",
    "!git clone https://github.com/tnc-ca-geo/animl-ml animl-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d51a1-66e6-4ca5-ad66-ed96dfe9df1a",
   "metadata": {},
   "source": [
    "\n",
    "#### Add additional directories\n",
    "\n",
    "Add additional directories (`~/classifier-training`, `~/images`, `~/crops`, etc.) so that the contents of your `home/` directory matches the following structure:\n",
    "\n",
    "```\n",
    "ai4eutils/                      # Microsoft's AI for Earth Utils repo\n",
    "\n",
    "animl-analytics/                # animl-analytics repo (utilities for exporting images)\n",
    "\n",
    "animl-ml/                       # This repo, contains Animl-specific utilities\n",
    "\n",
    "CameraTraps/                    # Microsoft's CameraTraps repo\n",
    "    classification/\n",
    "        BASE_LOGDIR/            # classification dataset and splits\n",
    "            LOGDIR/             # logs and checkpoints from a single training run\n",
    "\n",
    "classifier-training/            \n",
    "    mdcache/                    # cached \"MegaDetector\" outputs\n",
    "        v5.0b/                  #   NOTE: MegaDetector is in quotes because we're\n",
    "            datasetX.json       #   also storing Animl annotations here too\n",
    "    megaclassifier/             # files relevant to MegaClassifier\n",
    "\n",
    "crops/                          # local directory to save cropped images\n",
    "    datasetX/                   # images are organized by dataset\n",
    "        img0___crop00.jpg\n",
    "\n",
    "images/                         # local directory to save full-size images\n",
    "    datasetX/                   # images are organized by dataset\n",
    "        img0.jpg\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4921a49-eb65-405b-98c1-3d8dc57d8665",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "#### Setup Env variables\n",
    "The following environment variables are useful to have in `.bashrc`:\n",
    "\n",
    "```bash\n",
    "# Python development\n",
    "export PYTHONPATH=\"/home/<user>/CameraTraps:/home/<user>/ai4eutils\"\n",
    "export MYPYPATH=$PYTHONPATH\n",
    "```\n",
    "\n",
    "It's also helpful to set a `$BASE_LOGDIR` variable for the session:\n",
    "```bash\n",
    "export BASE_LOGDIR=\"/home/<user>/CameraTraps/classification/BASE_LOGDIR\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f34e8a-406b-4fd1-9087-2d30aee37a69",
   "metadata": {},
   "source": [
    "### AWS Configuration \n",
    "\n",
    "```bash\n",
    "!aws configure\n",
    "```\n",
    "```\n",
    "AWS Access Key ID [None]: enter your Key ID\n",
    "AWS Secret Access Key [None]: enter your Access Key\n",
    "Default region name [None]: us-west-2\n",
    "Default output format [None]: json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f31b49d-2855-4fc8-a6b0-0cbd2347ae13",
   "metadata": {},
   "source": [
    "Then we can test whether it worked by running the following command:\n",
    "``` \n",
    "aws s3 ls s3://animl-images-archive-prod \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d71a64-5db1-4ac0-a290-47babb4835ed",
   "metadata": {},
   "source": [
    "# Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23352e9a-f9eb-4814-96f6-2a5f067a988f",
   "metadata": {},
   "source": [
    "## 1. Select classification labels for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a339db16-a92e-43fe-b5a3-dba328619442",
   "metadata": {},
   "source": [
    "In the Animl Interfece, the following filters make sense when you're exporting the data:\n",
    "- fox\n",
    "- bird\n",
    "- skunk\n",
    "- rodent\n",
    "- lizard\n",
    "\n",
    "Then download the data with selected labels from the AniMl interface by clicking `EXPORT TO COCO` format. Then we get the cct.json file. Next, we can use it to download the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d1290-e9b6-47c5-acd6-10fe35498321",
   "metadata": {},
   "source": [
    "## 2. Download all the full size images referenced in the cct.json(COCO) file\n",
    "\n",
    "\n",
    "This code downloads image files from Amazon S3 and saves them to a local directory. \n",
    "\n",
    "The code takes two arguments: \"--coco-file\", the path to the coco file, and \"--output-dir\", the local directory to download the images to.\n",
    "\n",
    "The code uses the \"boto3\" library, which is the Amazon Web Services (AWS) SDK for Python, to access and interact with AWS services. The AWS profile and region are set as environment variables, and a session is established with boto3.\n",
    "\n",
    "The \"download_image_files\" function takes a list of image records, the destination directory, and the source bucket (defaulted to \"animl-images-archive-prod\"). It prints the number of image files being downloaded and downloads each image by using the \"download_file\" method of the S3 client.\n",
    "\n",
    "The \"load_json\" function loads the data from a file in JSON format.\n",
    "\n",
    "The code runs the \"download_image_files\" function if both the \"--coco-file\" and \"--output-dir\" arguments are provided. If either argument is missing, a message is displayed to the user to supply both arguments.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d059aec0-7e38-45b6-be9c-e564062ece3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ~/animl-analytics/utils/download_images.py \\\n",
    " --coco-file  ~/classifier-training/mdcache/v5.0b/<dataset_name>_cct.json\\\n",
    " --output-dir ~/images/<dataset_name>\n",
    "\n",
    "#remember to change the dataset_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21b11d-2743-44cc-9a59-dd99361cd7fb",
   "metadata": {},
   "source": [
    "## 3. Create a classification label specification JSON file(same format that MegaDetector outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63a7b4a-b468-4869-a0c4-1a9455de336b",
   "metadata": {},
   "source": [
    "Create a classification label specification JSON file (usually named label_spec.json). This file defines the labels that our classifier will be trained to distinguish, as well as the original dataset labels and/or biological taxa that will map to each classification label. \n",
    "\n",
    "Some of the following steps expect the image annotations to be in the same format that MegaDetector outputs after processing a batch of images. <b>To convert the COCO for Cameratraps file</b> that we exported from Animl to a MegaDetector results file, navigate to the /home/studio-lab-user/ directory and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94007699-9d71-4cf6-8d04-a97538bc08fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python animl-ml/classification/utils/cct_to_md.py \\\n",
    "  --input_filename ~/classifier-training/mdcache/v5.0b/<dataset_name>_cct.json \\\n",
    "  --output_filename ~/classifier-training/mdcache/v5.0b/<dataset_name>_md.json\n",
    "#remember to change the dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0660ec1-b680-4b31-b0c3-753832c70ea9",
   "metadata": {},
   "source": [
    "The code defines two functions: cct_to_md and _parse_args.\n",
    "\n",
    "cct_to_md takes two arguments, the input_filename which is the path to the cct file in json format and the output_filename which is the path to the output file in MegaDetector format.\n",
    "\n",
    "The function starts by checking if the input file exists, and if the output file name is not provided, it derives it from the input file name by adding \"_md-format\" before the file extension.\n",
    "\n",
    "The function then loads the contents of the input file into a dictionary d. It checks if the input file contains the required keys 'annotations', 'images' and 'categories', and if any of the keys is missing, it raises an error.\n",
    "\n",
    "The function then prepares metadata for the output file. It initializes a defaultdict image_id_to_annotations to store the annotations for each image. It also creates a dictionary category_id_to_name to store the mapping from category ID to category name.\n",
    "\n",
    "The function then loops over each image in the input file and for each image, it loops over its annotations. If the annotation has a bounding box (bbox), the code creates a detection dictionary and appends it to a list of detections. The detection dictionary contains the category name, confidence score, and the bounding box in MegaDetector format.\n",
    "\n",
    "Finally, the function writes the output to a file in the MegaDetector format.\n",
    "\n",
    "_parse_args is a helper function that defines and parses the command line arguments. It takes the input file path and the output file path as command line arguments and returns them as a Namespace object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1e2f09-4685-4e77-b660-57650c22bc3d",
   "metadata": {},
   "source": [
    "## 4. For images with ground-truth bounding boxes, generate bounding boxes using MegaDetector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d94632-a9d6-430b-949c-ddd0c80d4627",
   "metadata": {},
   "source": [
    "While some labeled images in MegaDB already have ground-truth bounding boxes, other images do not. For the labeled images without bounding box annotations, we run MegaDetector to get bounding boxes. MegaDetector can be run either locally or via the Batch Detection API.\n",
    "\n",
    "This step consists of 3 sub-steps:\n",
    "1. Run MegaDetector (either locally or via Batch API) on the queried images.\n",
    "2. Cache MegaDetector results on the images to JSON files in `classifier-training/mdcache`.\n",
    "3. Download and crop the images to be used for training the classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86ddfb-225d-4abc-a36d-f4fd2d43c771",
   "metadata": {},
   "source": [
    "### Crop images\n",
    "\n",
    "To crop images to their detections' respective bounding boxes, run:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c9611-d5b4-4ef2-a478-86f169253a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python animl-ml/classification/utils/crop_detections.py \\\n",
    "    ~/classifier-training/mdcache/v5.0b/<dataset_name>_md.json \\\n",
    "    ~/crops/<dataset_name> \\\n",
    "    --images-dir ~/images/<dataset_name> \\\n",
    "    --threshold 0 \\  # irrelevant for ground-truthed detections but we pass it in anyhow\n",
    "    --square-crops \\\n",
    "    --threads 50 \\\n",
    "    --logdir $BASE_LOGDIR\n",
    "#remember to change the dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f807d6df-3d50-46e7-baaa-5515f83017a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Debug \n",
    "!conda install azure-storage-blob\n",
    "!conda update libffi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4726b82-25dd-4d7f-9e57-c3ac202666d8",
   "metadata": {},
   "source": [
    "### Convert MegaDetector results file to queried_images.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8a0ccf-69a8-48c2-801f-efa275d9c7c1",
   "metadata": {},
   "source": [
    "Microsoft's CameraTraps/classification/create_classification_dataset.py takes the output of json_validator.py (see their docs on what that does here) as an input. To convert our MegaDetecotr results file to queried_images.json file, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e69dd3-0e0d-4bc5-85f4-63e5dd40d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "python animl-ml/classification/utils/md_to_queried_images.py \\\n",
    "  --input_filename ~/classifier-training/mdcache/v5.0b/<dataset_name>_md.json \\\n",
    "  --dataset <dataset_name> \\\n",
    "  --output_filename $BASE_LOGDIR/queried_images.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2faa41-8531-4f95-a32e-31c90a6ea6ec",
   "metadata": {},
   "source": [
    "The script `md_to_queried_images.py` is used to convert a MegaDetector output file (in JSON format) into a file that can be used as input for the `json_validator.py` script. It accepts three arguments:\n",
    "\n",
    "- input_filename: the path to the MegaDetector output file.\n",
    "- dataset: the name of the dataset.\n",
    "- output_filename: (optional) the filename for the output. If not provided, it will be created using the input file name.\n",
    "The script performs the following steps:\n",
    "\n",
    "1. Reads the MegaDetector output file.\n",
    "2. Filters out images that have more than one detection.\n",
    "3. Converts the MegaDetector output into the format required by json_validator.py.\n",
    "4. Writes the output to a file in JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207635cc-4aae-4019-a38c-3553750581f3",
   "metadata": {},
   "source": [
    "### Create classification dataset & split crops into train/val/test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ce86b-c70b-4c54-9eac-9e264042942e",
   "metadata": {},
   "source": [
    "This step is well documented in the `microsoft/CameraTraps/classification` [README](https://github.com/microsoft/CameraTraps/tree/main/classification#4-create-classification-dataset-and-split-image-crops-into-trainvaltest-sets-by-location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be097b-05b7-49db-a025-f7c180b709d9",
   "metadata": {},
   "source": [
    "\n",
    "Preparing a classification dataset for training involves two steps.\n",
    "\n",
    "1. Create a CSV file (`classification_ds.csv`) representing our classification dataset, where each row in this CSV represents a single training example, which is an image crop with its label. Along with this CSV file, we also create a `label_index.json` JSON file which defines a integer ordering over the string classification label names.\n",
    "2. Split the training examples into 3 sets (train, val, and test) based on the geographic location where the images were taken. The split is specified by a JSON file (`splits.json`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eb7df2-41bd-4cfa-bb95-609478b682b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python CameraTraps/classification/create_classification_dataset.py \\\n",
    "    $BASE_LOGDIR \\\n",
    "    --mode csv splits \\\n",
    "    --queried-images-json $BASE_LOGDIR/queried_images.json \\\n",
    "    --cropped-images-dir ~/crops \\\n",
    "    --detector-output-cache-dir ~/classifier-training/mdcache --detector-version 5.0b \\\n",
    "    --threshold 0 \\\n",
    "    --min-locs 3 \\\n",
    "    --val-frac 0.2 \\\n",
    "    --test-frac 0.2 \\\n",
    "    --method random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ecb8f-4189-4dd5-83bb-9c3e5f73f90d",
   "metadata": {},
   "source": [
    "1. Takes in various arguments such as output directory, mode (to create a CSV file or splits), test set, queried images, cropped images, detector version, confidence threshold, minimum locations, validation fraction, test fraction, splits method, and label specification.\n",
    "\n",
    "2. It reads the object detection results and creates a CSV file with information about the dataset, the location of the object in the image, and the label of the object.\n",
    "\n",
    "3. The code uses the create_classification_csv function to generate a CSV file with the information about the classification dataset. This function first filters the detections based on the confidence score, minimum locations, and test set locations. Then it crops the images and saves the information in the CSV file.\n",
    "\n",
    "4. The code also creates a label index JSON file that contains the names of all the labels in the dataset and their indices.\n",
    "\n",
    "5. If the mode includes creating splits, the code uses the create_splits_random function to split the data randomly into train, validation, and test splits, or create_splits_smallest_first to split the data in the smallest first manner, based on the selected splits method.\n",
    "\n",
    "6. The splits information is saved in the SPLITS_FILENAME.\n",
    "\n",
    "7. The code uses the tqdm library to show a progress bar during the creation of the dataset.\n",
    "\n",
    "This function appears to split a dataset into training, validation, and testing sets. The splitting of the dataset into these subsets is based on either random sampling or smallest-label-first. The random sampling split is created by the create_splits_random function, while the smallest-label-first split is created by the create_splits_smallest_label_first function.\n",
    "\n",
    "In the create_splits_random function, the dataset is first merged into a single string of 'dataset/location' and then transformed into a DataFrame that has the number of images for each label and location. This DataFrame is then used to randomly generate splits of the data into training, validation, and testing sets, where the fraction of data for each set is determined by the input arguments val_frac and test_frac. A score is calculated for each split, and the split with the lowest score is chosen as the final split. The score is calculated as the sum of the squared differences between the target fraction of images for each label and the actual fraction of images for each label in each split.\n",
    "\n",
    "In the create_splits_smallest_label_first function, the dataset is first transformed into a DataFrame that has the number of images for each label and location. The DataFrame is then sorted based on the number of images for each label, and the smallest label is added to the training set first. The process of adding labels to the training set is repeated until all the labels have been added to either the training, validation, or testing set. The fraction of the data for each set is determined by the input arguments val_frac and test_frac. A label specification file can also be provided to the function through the label_spec_json_path argument, which is a JSON file that maps each label to a set (training, validation, or testing).\n",
    "\n",
    "Both functions return a dictionary with keys 'train', 'val', and 'test' that map to lists of (dataset, location) tuples, where each tuple represents an image in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ceef4-5123-47ec-9873-1d432fd80183",
   "metadata": {},
   "source": [
    "### Start to Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79de41a-2e39-4ca0-8aba-9007ecc5495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train_classifier.py \\\n",
    "    $BASE_LOGDIR \\\n",
    "    ~/crops \\\n",
    "    --model-name efficientnet-b3 --pretrained \\\n",
    "    --label-weighted \\\n",
    "    --epochs 50 --batch-size 80 --lr 3e-5 \\ # I set batch-size 80 because \n",
    "    --weight-decay 1e-6 \\\n",
    "    --num-workers 4 \\ \n",
    "    --logdir $BASE_LOGDIR --log-extreme-examples 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "785990c0",
   "metadata": {},
   "source": [
    "### Get the result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7896368c",
   "metadata": {},
   "source": [
    "calculate the type I and type II error\n",
    "split,label,precision,recall\n",
    "* train,bird,0.9961076873175478, 0.977247414478918\n",
    "* train, fox,0.9919623603215055,0.998618511940004\n",
    "* train,lizard,0.9695334814344653,0.9938191281717632\n",
    "* train,rodent,0.9977037887485649,0.9949622166246851\n",
    "* train,skunk,0.9686162624821684,0.9970631424375918\n",
    "* val,bird,0.8967679691268693,0.8831353919239905\n",
    "* val,fox,0.8561484918793504,0.983344437041972\n",
    "* val,lizard,0.9367167919799498,0.943217665615142\n",
    "* val,rodent,0.9760956175298805,0.8808423215202876\n",
    "* val,skunk,0.9041916167664671,0.8435754189944135\n",
    "* test,bird,0.9653996101364523,0.9317968015051741\n",
    "* test,fox,0.9631038417649296,0.996066089693155\n",
    "* test,lizard,0.8991060025542784,0.9336870026525199\n",
    "* test,rodent,0.969416126042632,0.9199648197009674\n",
    "* test,skunk,0.9265175718849841,0.9764309764309764\n",
    "\n",
    "\n",
    "#calculate the type I and type II error if we have precision and recall \n",
    "type_I_error = 1 - precision\n",
    "type_II_error = 1 - recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad07487",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6861dab717bb2fb6655ca8a67b6affd5347bd17adf9b2dd60468dfa4b006dd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
